{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93541f0c",
   "metadata": {},
   "source": [
    "## Querying Streaming Spark DataFrames in an EMR Notebook\n",
    "\n",
    "In this notebook, we will read data from a modified version of the Kinesis stream from Lab 5 into a Spark streaming DataFrame. Once we've loaded our streaming DataFrame, we'll perform a simple query on it and write the results of our query to S3 for further analysis.\n",
    "\n",
    "We've modified the producer from Lab 5 to send tweet-like JSON data into our `test_stream` Kinesis stream in the form of `{\"username\": ..., \"age\": ..., \"num_followers\": ..., \"tweet\": ...}` (by adding additional test data for `age` and `num_followers`). If you're following along with the code in this notebook, be sure to use a similar producer script to put data into a Kinesis stream:\n",
    "\n",
    "```\n",
    "import boto3\n",
    "import testdata\n",
    "import json\n",
    "\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "# Continously write Twitter-like data into Kinesis stream\n",
    "while 1 == 1:\n",
    "    test_tweet = {'username': testdata.get_username(),\n",
    "                  'age': testdata.get_int(18, 100),\n",
    "                  'num_followers': testdata.get_int(0, 10000),\n",
    "                  'tweet':    testdata.get_ascii_words(280)\n",
    "                  }\n",
    "    kinesis.put_record(StreamName = \"test_stream\",\n",
    "                       Data = json.dumps(test_tweet),\n",
    "                       PartitionKey = \"partitionkey\"\n",
    "                      )\n",
    "```\n",
    "\n",
    "First, let's add the [Spark Structured Streaming package](https://spark.apache.org/docs/2.4.7/structured-streaming-programming-guide.html) to our session configuration (we'll specifically add a version that makes it possible to interact with Kinesis streams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f174dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\"spark.jars.packages\": \"com.qubole.spark/spark-sql-kinesis_2.11/1.1.3-spark_2.4\" }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd35cd",
   "metadata": {},
   "source": [
    "Then, we're ready to start reading from our Kinesis stream. For this demonstration, we'll start with the latest data in the stream, but we could get more granular if we would like to do so as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43beb294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7d52b9d2c543dfbe8fdd0faaeaee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1620656441511_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-214.ec2.internal:20888/proxy/application_1620656441511_0014/\" class=\"emr-proxy-link\" emr-resource=\"j-3QJ7YCEW096AW\n",
       "\" application-id=\"application_1620656441511_0014\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-20-214.ec2.internal:8042/node/containerlogs/container_1620656441511_0014_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "DataFrame is streaming"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, json_tuple\n",
    "import time\n",
    "\n",
    "stream_df = spark.readStream \\\n",
    "                 .format('kinesis') \\\n",
    "                 .option('streamName', 'test_stream') \\\n",
    "                 .option('endpointUrl', 'https://kinesis.us-east-1.amazonaws.com')\\\n",
    "                 .option('region', 'us-east-1') \\\n",
    "                 .option('startingposition', 'LATEST')\\\n",
    "                 .load()\n",
    "\n",
    "if stream_df.isStreaming:\n",
    "    print('======================')\n",
    "    print('DataFrame is streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce2a28",
   "metadata": {},
   "source": [
    "Now that we have our streaming DataFrame ready, let's use Spark SQL `select` and `where` methods to query our streaming DataFrame. We'll then write this data out to one of an S3 bucket (you'll need to specify your own and then append it with `/data` and `/checkpoints` directories to follow along). Individual CSVs will be produced for each set of data that is processed in a micro-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a697c944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80acf397249c4d71924d15b417143965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start process of querying streaming data\n",
    "query = stream_df.selectExpr('CAST(data AS STRING)', 'CAST(approximateArrivalTimestamp as TIMESTAMP)') \\\n",
    "    .select('approximateArrivalTimestamp', \n",
    "            json_tuple(col('data'), 'username', 'age', 'num_followers', 'tweet'\n",
    "                      ).alias('username', 'age', 'num_followers', 'tweet')) \\\n",
    "    .select('approximateArrivalTimestamp', 'username', 'age') \\\n",
    "    .where('age > 35') \\\n",
    "    .writeStream \\\n",
    "    .queryName('counts') \\\n",
    "    .outputMode('append') \\\n",
    "    .format('csv') \\\n",
    "    .option('path', 's3://mrjob-9caa69460249cdb9/data') \\\n",
    "    .option('checkpointLocation','s3://mrjob-9caa69460249cdb9/checkpoints') \\\n",
    "    .start()\n",
    "\n",
    "# let streaming query run for 15 seconds (and continue sending results to CSV in S3), then stop it\n",
    "time.sleep(15)\n",
    "\n",
    "# Stop query; look at results of micro-batch queries in S3 bucket in `/data` directory\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c0e66",
   "metadata": {},
   "source": [
    "Cool! If we take a look at one of our resulting CSVs over in our S3 bucket (see head below), we can see that it produces the expected results (a selection of columns from the streaming data that is filtered by age). This is a great way to quickly process streaming data!\n",
    "\n",
    "```\n",
    "2021-05-10T22:03:40.787Z,Hailiejade,83\n",
    "2021-05-10T22:03:40.824Z,Fischer,79\n",
    "2021-05-10T22:03:40.866Z,Leonard,46\n",
    "2021-05-10T22:03:40.902Z,Vasquez,65\n",
    "2021-05-10T22:03:40.937Z,Porter,86\n",
    "2021-05-10T22:03:40.978Z,Joan,56\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
